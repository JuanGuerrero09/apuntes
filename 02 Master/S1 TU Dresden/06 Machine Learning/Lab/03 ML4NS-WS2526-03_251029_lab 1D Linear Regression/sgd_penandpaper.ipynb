{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aad906c",
   "metadata": {},
   "source": [
    "# A multi-layer perceptron with pen and paper\n",
    "\n",
    "Multi-layer Perceptrons (MLP) were some of the first Machine Learning (ML) architectures in use.\n",
    "\n",
    "This notebook will guide you through how to\n",
    "- perform a forward pass through a very simple MLP\n",
    "- how to backpropagate through the network\n",
    "- how to perform the same operations with [pytorch](https://pytorch.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e247188",
   "metadata": {},
   "source": [
    "## A FeedForward MLP\n",
    "\n",
    "Let's recap the essential ingredients of a feedforward neural network:\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 75%;\"><img src=\"img/01_plain_mlp.svg\" alt=\"plain MLP\"></div>\n",
    "\n",
    "In the above, the network receives an input datum $x$. This is used as input to the first hidden unit $\\sigma (\\vec{W} \\cdot \\vec{x} + \\vec{b})$. The hidden unit consists of 3 ingredients:\n",
    "- a weight matrix $W$\n",
    "- a bias vector $b$\n",
    "- an activation function $\\sigma$\n",
    "\n",
    "The output of the hidden unit will be considered to be the input of the last layer. This in turn produces the output $\\vec{\\hat{y}}$ of the forward pass. In order to establish a learning process, this prediction $\\hat{y}$ will be compared with a label $y$ using the loss the function $\\mathcal{L}(y,\\hat{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644faaba",
   "metadata": {},
   "source": [
    "> **A FeedForward MLP with all details**\n",
    ">\n",
    "> For you to dive into all details, here is a pictorial representation of a MLP with all math operations spelled out.\n",
    ">\n",
    "> <div style=\"display: block;margin-left: auto;margin-right: auto;width: 75%;\"><img src=\"img/01_detailed_mlp.svg\" alt=\"detailed MLP\"></div>\n",
    ">\n",
    "> To read through it, take your hand to block out the right hand part of the network. Then follow the flow of datum $x_0$ through the hidden layers until the output layers. Once you are done following $x_0$, have a look at $x_1$. You will notice that the operations are the same, except the input.\n",
    ">\n",
    "> Some comments to the above:\n",
    "> - the figure omits the display of all matrix elements of $W^{I}$ and $W^{II}$ to remain clear for the reader\n",
    "> - the last layer does not explicitely state which activation is used. Depending on the use case, this activation function can be a [ReLU](https://en.wikipedia.org/wiki/ReLU) or [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) (for regression) or a [softmax](https://en.wikipedia.org/wiki/Softmax_function) function (for classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384fe65f",
   "metadata": {},
   "source": [
    "**Exercise 01.1**\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 75%;\"><img src=\"img/01_detailed_mlp.svg\" alt=\"linear MLP with values\"></div>\n",
    "\n",
    "Take pen and paper or a digital equivalent. Mark the path of all computations which result in $y_{1}$! Write down the weight matrix elements which will be used to compute this output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290bdd3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 2
   },
   "source": [
    "**Solution 01.1**\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 75%;\"><img src=\"img/01_detailed_mlp.svg.svg\" alt=\"linear MLP with values\"></div>\n",
    "\n",
    "All weight matrix elements will be used except the following 3 elements $W_{00}^{II}, W_{10}^{II}, W_{10}^{II}$ as their are required to calculate $\\hat{y}_0$. All results of the hidden layer will be used as inputs to the last layer, that is why all elements of $W^{I}$ are part of the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055ff6f",
   "metadata": {},
   "source": [
    "## A simple MLP\n",
    "\n",
    "To start off this exericse, we want to construct a very simple MLP with one input unit, one hidden unit and one ouput unit. We will keep everything lightweight and one dimensional.\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 75%;\"><img src=\"img/01_1D_mlp.svg\" alt=\"1D MLP\"></div>\n",
    "\n",
    "In the above, the network receives an input datum $x$. This is used as input to the first hidden unit $\\sigma (w \\cdot x' + b)$. The hidden unit consists of 3 ingredients:\n",
    "- the weight $w$\n",
    "- the bias $b$\n",
    "- the activation function $\\sigma$\n",
    "\n",
    "The output of the hidden unit will be considered to be the output of the forward pass. In order to establish a learning process, this prediction $y'$ will be compared with a label $y$ using the loss the function $\\mathcal{L}(y,y')$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03855e97",
   "metadata": {},
   "source": [
    "**Exercise 01.2**\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 75%;\"><img src=\"img/01_1D_mlp_filled.svg\" alt=\"linear MLP with values\"></div>\n",
    "\n",
    "Take pen and paper. Compute a full forward pass using the following values:\n",
    "\n",
    "- input $x = 2$\n",
    "- weight $w = .5$\n",
    "- bias $b = 1$\n",
    "- label $y = 1.5$\n",
    "\n",
    "Use the [ReLU function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) for $\\sigma$ and the mean squared error ([MSE](https://en.wikipedia.org/wiki/Mean_squared_error)) for the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab77614",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> **Solution 01.2**\n",
    "> 1. Compute $w \\cdot x' + b$: We obtain `2`.\n",
    "> 2. Apply the ReLU: We obtain `y'=2` again (2 is larger than 0 and hence $f_{ReLU}(x=2)=2$).\n",
    "> 3. Compute the loss: $\\mathcal{L}(y,y') = (y-y')^2 = (2 - 1.5)^2 = \\frac{1}{4}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416d993",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "Given a dataset $\\mathcal{D} = \\{\\langle \\vec{x}_i, y_i\\rangle \\dots \\}$ with input data $x \\in \\mathbb{R}^n$ and labels $y \\in \\mathbb{R}^{2}$, we would like to train a model $f$ with parameters $\\vartheta = \\{ W^{I}, \\vec{b}^{I}, W^{II}, \\vec{b}^{II} \\}$ such that:\n",
    "\n",
    "$$ \\vec{\\hat{y}} = \\hat{f}(\\vec{x}|\\vartheta) $$\n",
    "\n",
    "To obtain a good estimate of $f$, we alter the weights of our model $\\vartheta$. To do this, the optimisation is performed using a loss function $\\mathcal{L}$ to obtain an optimal set of weights $\\vartheta$ by minimizing $\\mathcal{L}$:\n",
    "\n",
    "$$ \\vartheta \\approx \\text{argmin}_{\\vartheta} \\mathcal{L}( \\vec{y}, \\vec{\\hat{y}} = f(\\vec{x} | \\vartheta) ) $$.\n",
    "\n",
    "The optimisation is performed using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). In this optimisation scheme, we update the parameters $\\vartheta$ in a step-by-step fashion. After being randomly initialized, the parameters are updated at step $s$ using the weight update rule:\n",
    "\n",
    "$$ \\vartheta_{s+1} = \\vartheta_{s} + \\eta \\nabla_{\\vartheta} \\mathcal{L}( \\vec{y}, f(\\vec{x}|\\vartheta_{s})) $$\n",
    "\n",
    "The above equation is called __weight update rule__. Here, the free parameter $\\eta$ is also known as the __learning rate__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2734ea",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "The central part of gradient descent is how to obtain the value of $\\nabla_{\\vartheta}\\mathcal{L}$. This is performed by applying the chain rule of differentiation from the back of the model to the front. For the sake of simplicity, let's say we only have part of a model: $f'(x|\\vartheta) = w \\cdot x + b = \\hat{y}$. To compute the gradient of this simple model $f'$, we have to start from the loss function $\\mathcal{L}(y,\\hat{y})= (y-\\hat{y})^2$:\n",
    "\n",
    "$$ \\nabla \\mathcal{L} = \\frac{\\partial\\mathcal{L}}{\\partial\\vartheta} $$\n",
    "\n",
    "The gradient $\\nabla$ is calculated for each weight or bias term independently (as they are also used independently during the forward pass).\n",
    "\n",
    "$$ \\nabla_w \\mathcal{L} = \\frac{\\partial\\mathcal{L}}{\\partial w} $$\n",
    "\n",
    "We can now go forward and apply the chain rule to the differential above:\n",
    "\n",
    "$$ \\nabla_w \\mathcal{L} = \\frac{\\partial\\mathcal{L}}{\\partial w} = \\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial w}$$\n",
    "\n",
    "We can now move forward and evaluate each term of the chain rule expression:\n",
    "\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}} = \\frac{\\partial (y-\\hat{y})^2}{\\partial \\hat{y}} = 2\\cdot(y-\\hat{y})\\cdot(-1)$$\n",
    "$$ \\frac{\\partial \\hat{y}}{\\partial w} = \\frac{\\partial (w \\cdot x + b)}{\\partial w} = x $$\n",
    "\n",
    "If we now want to compute the value of the gradient, we would have to input concrete numbers to finally apply the weight update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e4679",
   "metadata": {},
   "source": [
    "**Exercise 01.3**\n",
    "\n",
    "Take pen and paper or a digital equivalent. Compute the gradient for $b$, $\\nabla_b \\mathcal{L}$ of our stub model $f'$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb242d8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> **Solution 01.3**\n",
    "> 1. Apply the chain rule to our stub network:\n",
    "> $$\\frac{\\partial\\mathcal{L}}{\\partial b} = \\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial b}$$\n",
    "> 2. Evaluate each subterm:\n",
    "> $$ \\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}} = \\frac{\\partial (y-\\hat{y})^2}{\\partial \\hat{y}} = 2\\cdot(y-\\hat{y})\\cdot(-1)$$\n",
    "> $$ \\frac{\\partial \\hat{y}}{\\partial b} = \\frac{\\partial (w \\cdot x + b)}{\\partial b} = 1 $$\n",
    "> 3. Put everything together:\n",
    "> $$ \\nabla_b \\mathcal{L} = \\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial b} = 2\\cdot(y-\\hat{y})\\cdot(-1) \\cdot 1 = -2\\cdot(y-\\hat{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e0604",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "We now want to perform some stochastic gradient based optimisation by hand to conclude this pen&paper part of the exercise.\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 75%;\"><img src=\"img/01_1D_2hidden_mlp.svg\" alt=\"1D MLP\"></div>\n",
    "\n",
    "To make this worth your time, we split the audience into groups. You will get distinct data sets. Please calculate the one iteration of gradient descent for your parameter set and check if gradient descent gets you closer to the label.\n",
    "\n",
    "**Exercise 01.4**\n",
    "\n",
    "Groups:\n",
    "1. $x=2$, $w_0=4$, $b_0=2$, $w_1=.75$, $b_1=.5$, $y=5$\n",
    "2. $x=2$, $w_0=2$, $b_0=.25$, $w_1=5$, $b_1=0$, $y=5$\n",
    "3. $x=2$, $w_0=-4$, $b_0=2$, $w_1=.75$, $b_1=-.5$, $y=5$\n",
    "4. $x=2$, $w_0=2$, $b_0=-.25$, $w_1=-5$, $b_1=0$, $y=5$\n",
    "5. $x=2$, $w_0=1$, $b_0=1$, $w_1=1$, $b_1=1$, $y=5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70086b62",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "To par our pen & paper exercises with an introduction to one central library, we want to start off and compute the last exercise **01.4** in pytorch. This will also give us a chance to the check the results. On top, the central mechanisms of pytorch can be exposed which directly leads us into further topics of classification and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c2265",
   "metadata": {},
   "source": [
    "## Our dataset\n",
    "\n",
    "We start out by defining the input data and the outputs. We will use the data of Group 1 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b3e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor([[2.]])\n",
    "y = torch.Tensor([[5.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26466fc1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "The central building block of `pytorch` is a `torch.Tensor` object. The API of `Tensor` is very similar to that of a `numpy.ndarray`. That makes it easier to switch between libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65168e2d",
   "metadata": {},
   "source": [
    "## Our model\n",
    "\n",
    "We start out by defining the input data and the outputs. We will use the data of Group 1 as an example.\n",
    "To define a neural network, the mechanics of pytorch require us to define a class. This class needs to be derived from `torch.nn.Module`. Within the class, we have to define the `forward` function which is effectively the forward pass of our model and provide a constructor method `__init__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d8b54",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class f_prime_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #for more complicated models, this constructor will be rather complicated\n",
    "        self.hidden0 = torch.nn.Linear(in_features=1, out_features=1)\n",
    "        self.relu0 = torch.nn.ReLU()\n",
    "        self.hidden1 = torch.nn.Linear(in_features=1, out_features=1)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" forward pass of our model, using x as the input data \"\"\"\n",
    "        h = self.hidden0(x)\n",
    "        h_ = self.relu0(h)\n",
    "        y_hat_ = self.hidden1(h_)\n",
    "        y_hat = self.relu1(y_hat_)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae264bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's instantiate the model\n",
    "model = f_prime_model()\n",
    "\n",
    "# we want to start at fixed values; as pytorch will automatically hook the model class up to it's capabilities to compute the gradient internally, we have to forbid this from happening here using the torch.no_grad context manager:\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.hidden0.weight.fill_(4.)\n",
    "    model.hidden0.bias.fill_(2.)\n",
    "    model.hidden1.weight.fill_(0.75)\n",
    "    model.hidden1.bias.fill_(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can convince ourselves that we achieved to set the internal values of our model\n",
    "for param in model.named_parameters():\n",
    "    name, value = param\n",
    "    print(name, value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98871733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A first feed forward pass can be invoced like this! Note the syntax model(x) effectively calls model.forward(x).\n",
    "\n",
    "y_hat = model(x)\n",
    "print(y_hat.item(), y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a637cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to set up our loss function. As we are interested in regression, we use the mean squared error [MSE](https://en.wikipedia.org/wiki/Mean_squared_error).\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd7e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we want to set up the optimizer to perform one learning step. The constructor requires to provide the model parameters. This is the \"bridge\" to connect model with optimizer for the later optimization routine.\n",
    "opt = torch.optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we perform one feedforward pass and one backward pass\n",
    "\n",
    "y_hat = model(x)\n",
    "loss = loss_fn(y_hat, y) #loss function computed (computational graph is internally established)\n",
    "loss.backward() #backpropagate through loss function\n",
    "opt.step() #weight updated step in model.paramters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa3a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first check if the model paramters changed\n",
    "\n",
    "for i in model.named_parameters():\n",
    "    name, value = i\n",
    "    print(name, value.item())\n",
    "\n",
    "# indeed they did change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c410ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also check if the prediction changed\n",
    "print(model(x).item())\n",
    "# indeed it did change too!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
